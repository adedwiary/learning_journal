{"cells":[{"cell_type":"markdown","metadata":{},"source":["![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F3335785%2F219228e436b0e694f835f70194e45c8c%2Fmaxresdefault.jpg?generation=1589357922679234&alt=media)\n","\n","<font size='5' color='blue' align = 'center'>Table of Contents</font> \n","<font size='3' color='purple'>\n","1. [Introduction](#1)\n","    \n","1. [**Ensemble Techniques**](#2)\n","    \n","    2.1 [**Max Voting / Voting Classifier**](#21)\n","    \n","    2.2 [**Averaging**](#22)\n","    \n","    2.3 [**Weighted Averaging**](#23)\n","    \n","    2.4 [**Stacking**](#24)\n","    \n","    2.5 [**Blending**](#25)\n","    \n","    2.6 [**Bagging**](#26)\n","    \n","    2.7 [**Boosting**](#27)\n"," \n","1. [References](#3)  \n","\n","1. [Conclusion](#4)  \n","\n","    \n","# 1. Introduction <a id=\"1\"></a> <br>\n","    \n","Misalkan Anda ingin membeli mobil. Sekarang dengan mengunjungi perusahaan mobil pertama dan berdasarkan saran dealer apakah kita akan langsung melakukan pembelian mobil? Jawabannya pasti TIDAK besar kan?\n","    \n","![](https://thumbs.dreamstime.com/b/car-sale-4167169.jpg)\n","    \n","Jadi yang kita lakukan adalah memutuskan dulu mobil mana yang akan dibeli, apakah itu mobil baru atau bekas, jenis mobil, model dan tahun pembuatannya, mencari daftar dealer, mencari diskon/penawaran, ulasan pelanggan, pendapat dari teman dan keluarga, kinerja, efisiensi bahan bakar dan jelas setiap pembeli mobil akan untuk kisaran harga terbaik dll.\n","    \n","![](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcSC0-mqf3xqr3MESGW-mGwaWQkkBjwJGbNFsQ&usqp=CAU)\n","    \n","Singkatnya, Anda tidak akan langsung mencapai kesimpulan, tetapi akan membuat keputusan dengan mempertimbangkan semua faktor yang disebutkan di atas sebelum kami memutuskan pilihan terbaik.\n","    \n","**Ensemble models** in machine learning operate on a similar idea.\n","![](https://i.pinimg.com/474x/d7/c7/9b/d7c79b0c7abc5a34e17710fe596f6834.jpg)    \n","Ensemble Learning membantu meningkatkan hasil pembelajaran mesin dengan menggabungkan beberapa model untuk meningkatkan kinerja prediktif dibandingkan dengan satu model.\n","![](https://i.imgur.com/L2Jaqm8.png)\n","    \n","# 2. Ensemble Techniques <a id=\"2\"></a> <br>\n","\n","## 2.1 Max Voting / Voting Classifier <a id=\"2.1\"></a> <br>\n","\n","Metode **max voting** umumnya digunakan untuk masalah klasifikasi. Dalam teknik ini, beberapa model digunakan untuk membuat prediksi untuk setiap titik data. Prediksi oleh masing-masing model dianggap sebagai 'vote'. Prediksi yang kami dapatkan dari sebagian besar model digunakan sebagai prediksi akhir.\n","\n","**Voting Classifier** adalah model pembelajaran mesin yang melatih ensemble dari banyak model dan memprediksi output (kelas) berdasarkan probabilitas tertinggi dari kelas yang dipilih sebagai output.\n","Ini hanya mengumpulkan temuan dari setiap pengklasifikasi yang diteruskan ke Pengklasifikasi Pemungutan Suara dan memprediksi kelas keluaran berdasarkan mayoritas suara terbanyak. Idenya adalah alih-alih membuat model khusus yang terpisah dan menemukan akurasi untuk masing-masing model, kami membuat model tunggal yang dilatih oleh model ini dan memprediksi keluaran berdasarkan mayoritas gabungan suara untuk setiap kelas keluaran.\n","\n","Sekarang mari kita gunakan dataset IRIS untuk mendemonstrasikan Pengklasifikasi Voting"]},{"cell_type":"code","execution_count":7,"metadata":{"_kg_hide-input":true,"trusted":true},"outputs":[],"source":["# importing libraries \n","from sklearn.ensemble import VotingClassifier ,BaggingClassifier, ExtraTreesClassifier, RandomForestClassifier\n","from sklearn.ensemble import AdaBoostClassifier,GradientBoostingClassifier\n","from sklearn.metrics import accuracy_score \n","from numpy import mean,std\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import cross_val_score,RepeatedStratifiedKFold,train_test_split\n","from sklearn.linear_model import LogisticRegression,RidgeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.svm import SVC\n","from sklearn.naive_bayes import GaussianNB\n","from matplotlib import pyplot\n","from sklearn.datasets import load_wine,load_iris\n","from matplotlib.pyplot import figure\n","figure(num=2, figsize=(16, 12), dpi=80, facecolor='w', edgecolor='k')\n","import xgboost as xgb\n","from sklearn.feature_selection import SelectKBest,f_regression\n","from sklearn.linear_model import LinearRegression,BayesianRidge,ElasticNet,Lasso,SGDRegressor,Ridge\n","from sklearn.kernel_ridge import KernelRidge\n","from sklearn.preprocessing import LabelEncoder,OneHotEncoder,RobustScaler,StandardScaler\n","from sklearn.pipeline import make_pipeline,Pipeline\n","from sklearn.metrics import mean_squared_error\n","from sklearn.decomposition import PCA,KernelPCA\n","from sklearn.ensemble import ExtraTreesRegressor,GradientBoostingRegressor,RandomForestRegressor,VotingClassifier\n","from sklearn.model_selection import cross_val_score,KFold,GridSearchCV,RandomizedSearchCV,StratifiedKFold,train_test_split\n","from sklearn.base import BaseEstimator,clone,TransformerMixin,RegressorMixin\n","from sklearn.svm import LinearSVR,SVR\n","#import xgboost \n","from xgboost import XGBRegressor\n","#Import Pandas\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib\n","import seaborn as sns\n","import warnings\n","warnings.filterwarnings('ignore')\n","from scipy.stats import skew\n","from scipy.stats.stats import pearsonr\n","%matplotlib inline\n","seed = 1075\n","np.random.seed(seed)"]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[],"source":["# loading iris dataset \n","iris = load_iris() \n","X = iris.data[:, :4] \n","Y = iris.target "]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size = 0.20,random_state = 42) "]},{"cell_type":"code","execution_count":10,"metadata":{"trusted":true},"outputs":[],"source":["# Ensemble of Models \n","estimator = [] \n","estimator.append(('LR',LogisticRegression(solver ='lbfgs',multi_class ='multinomial',max_iter = 200))) \n","estimator.append(('SVC', SVC(gamma ='auto', probability = True))) \n","estimator.append(('DTC', DecisionTreeClassifier())) "]},{"cell_type":"markdown","metadata":{},"source":["**Voting Classifier** mendukung dua jenis voting.\n","\n","**Hard Voting:** Dalam hard voting, kelas keluaran yang diprediksi adalah kelas dengan suara terbanyak, yaitu kelas yang memiliki probabilitas tertinggi untuk diprediksi oleh masing-masing pengklasifikasi. Misalkan tiga pengklasifikasi memprediksi kelas output (A, A, B), jadi di sini mayoritas memprediksi A sebagai output. Oleh karena itu A akan menjadi prediksi akhir.\n","\n","![](https://image.slidesharecdn.com/7-180514114334/95/ensemble-learning-and-random-forests-12-638.jpg?cb=1527755412)"]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[],"source":["# Voting Classifier with hard voting \n","hard_voting = VotingClassifier(estimators = estimator, voting ='hard') \n","hard_voting.fit(X_train, y_train) \n","y_pred = hard_voting.predict(X_test)   "]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Hard Voting Score  1\n"]}],"source":["# accuracy_score metric to predict Accuracy \n","score = accuracy_score(y_test, y_pred) \n","print(\"Hard Voting Score % d\" % score) "]},{"cell_type":"markdown","metadata":{},"source":["**Soft Voting:** Dalam soft voting, kelas keluaran adalah prediksi berdasarkan rata-rata probabilitas yang diberikan pada kelas tersebut. Misalkan diberikan beberapa masukan untuk tiga model, probabilitas prediksi untuk kelas A = (0,30, 0,47, 0,53) dan B = (0,20, 0,32, 0,40). Jadi rata-rata untuk kelas A adalah 0,4333 dan B adalah 0,3067, pemenangnya jelas kelas A karena memiliki probabilitas tertinggi yang dirata-ratakan oleh masing-masing pengklasifikasi."]},{"cell_type":"code","execution_count":13,"metadata":{"trusted":true},"outputs":[],"source":["# Voting Classifier with soft voting \n","soft_voting = VotingClassifier(estimators = estimator, voting ='soft') \n","soft_voting.fit(X_train, y_train) \n","y_pred = soft_voting.predict(X_test) "]},{"cell_type":"code","execution_count":14,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Soft Voting Score  1\n"]}],"source":["# Using accuracy_score \n","score = accuracy_score(y_test, y_pred) \n","print(\"Soft Voting Score % d\" % score) "]},{"cell_type":"markdown","metadata":{},"source":["Dalam praktiknya, akurasi output akan lebih untuk soft voting karena ini adalah probabilitas rata-rata dari semua estimator yang digabungkan, karena untuk dataset iris dasar kami, kami sudah overfitting, jadi tidak akan ada banyak perbedaan dalam output.\n","\n","## 2.2 Averaging <a id=\"2.2\"></a> <br>\n","\n","Beberapa prediksi dibuat untuk setiap titik data dalam rata-rata. Dalam metode ini, kami mengambil rata-rata prediksi dari semua model dan menggunakannya untuk membuat prediksi akhir. Rata-rata dapat digunakan untuk membuat prediksi dalam masalah regresi atau saat menghitung probabilitas untuk masalah klasifikasi.\n","\n","Cara paling sederhana untuk mengembangkan model averaging ensemble di Keras adalah dengan melatih beberapa model pada dataset yang sama kemudian menggabungkan prediksi dari masing-masing model yang dilatih.\n","\n","Kami akan menggunakan masalah klasifikasi multi-kelas kecil sebagai dasar untuk mendemonstrasikan ansambel rata-rata model.\n","\n","Kelas scikit-learn menyediakan fungsi make_blobs() yang dapat digunakan untuk membuat masalah klasifikasi multi-kelas dengan jumlah sampel yang ditentukan, variabel input, kelas, dan varians sampel di dalam kelas.\n","\n","Kami menggunakan masalah ini dengan 500 contoh, dengan variabel input untuk mewakili koordinat x dan y dari titik dan standar deviasi 2,0 untuk titik dalam setiap grup. Kami akan menggunakan status acak yang sama untuk memastikan bahwa kami selalu mendapatkan 500 poin yang sama."]},{"cell_type":"code","execution_count":15,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.datasets import make_blobs\n","from matplotlib import pyplot\n","from pandas import DataFrame"]},{"cell_type":"code","execution_count":16,"metadata":{"trusted":true},"outputs":[],"source":["# generate 2d classification dataset\n","X, y = make_blobs(n_samples=500, centers=3, n_features=2, cluster_std=2, random_state=2)"]},{"cell_type":"code","execution_count":17,"metadata":{"trusted":true},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["# scatter plot, dots colored by class value\n","df = DataFrame(dict(x=X[:,0], y=X[:,1], label=y))\n","colors = {0:'red', 1:'blue', 2:'green'}\n","fig, ax = pyplot.subplots()\n","grouped = df.groupby('label')\n","for key, group in grouped:\n","    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n","pyplot.show()"]},{"cell_type":"markdown","metadata":{},"source":["Kita dapat melihat bahwa standar deviasi 2.0 berarti bahwa kelas-kelas tidak dapat dipisahkan secara linier (dipisahkan oleh garis) menyebabkan banyak titik ambigu.\n","\n","**Jadi, secara ringkas untuk setiap contoh kumpulan data pengujian, prediksi rata-rata dihitung. Metode ini sering kali mengurangi overfit dan menciptakan model regresi yang lebih halus.**\n","\n","## 2.3 Weighted Averaging <a id=\"2.3\"></a> <br>\n","Ini adalah perpanjangan dari metode rata-rata. Semua model diberi bobot berbeda yang menentukan pentingnya setiap model untuk prediksi.\n","\n","![](https://www.analyticsvidhya.com/wp-content/uploads/2015/08/Screen-Shot-2015-08-22-at-6.40.37-pm.png)\n","\n","Untuk ini kami akan menggunakan dataset harga perumahan untuk menunjukkan seperti yang ditunjukkan di bawah ini\n","\n","Pertama-tama impor perpustakaan & data"]},{"cell_type":"code","execution_count":20,"metadata":{"trusted":true},"outputs":[],"source":["train = pd.read_csv('dataset/house-prices-advanced-regression-techniques/train.csv',na_values = '#NAME?')\n","test = pd.read_csv('dataset/house-prices-advanced-regression-techniques/test.csv',na_values = '#NAME?')"]},{"cell_type":"markdown","metadata":{},"source":["Berdasarkan distribusi data, mari kita hapus beberapa outlier"]},{"cell_type":"code","execution_count":21,"metadata":{"trusted":true},"outputs":[],"source":["train.drop(train[(train['GrLivArea'] >4000) & (train['SalePrice']<300000)].index,inplace = True)"]},{"cell_type":"markdown","metadata":{},"source":["Mari kita gabungkan set data pelatihan dan pengujian ke dalam kerangka data tunggal untuk kemudahan pembersihan data dan rekayasa fitur. Fitur 'Id' tidak memiliki arti penting bagi pemodelan kita karena merupakan variabel kontinu, jadi hapus fitur ini pada set data latih dan uji."]},{"cell_type":"code","execution_count":22,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["(2917, 80)"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["full = pd.concat([train,test],ignore_index=True)\n","full.drop('Id',axis = 1,inplace = True)\n","full.shape"]},{"cell_type":"markdown","metadata":{},"source":["Sekarang mari kita praproses data dengan melakukan beberapa perlakuan nilai yang hilang"]},{"cell_type":"code","execution_count":23,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["PoolQC          2908\n","MiscFeature     2812\n","Alley           2719\n","Fence           2346\n","SalePrice       1459\n","FireplaceQu     1420\n","LotFrontage      486\n","GarageCond       159\n","GarageYrBlt      159\n","GarageFinish     159\n","GarageQual       159\n","GarageType       157\n","BsmtExposure      82\n","BsmtCond          82\n","BsmtQual          81\n","BsmtFinType2      80\n","BsmtFinType1      79\n","MasVnrType        24\n","MasVnrArea        23\n","MSZoning           4\n","Functional         2\n","BsmtHalfBath       2\n","BsmtFullBath       2\n","Utilities          2\n","KitchenQual        1\n","TotalBsmtSF        1\n","BsmtUnfSF          1\n","GarageCars         1\n","GarageArea         1\n","BsmtFinSF2         1\n","BsmtFinSF1         1\n","Exterior2nd        1\n","Exterior1st        1\n","SaleType           1\n","Electrical         1\n","dtype: int64"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["missing_values = full.isnull().sum()\n","missing_values[missing_values>0].sort_values(ascending = False)"]},{"cell_type":"markdown","metadata":{},"source":["mari kita hitung nilai yang hilang dari LotFrontage berdasarkan median LotArea dan Neighborhood. Untuk mencapai ini, mari kita kelompokkan Neighborhood dan LotFrontage terlebih dahulu sehubungan dengan median, mean dan count."]},{"cell_type":"code","execution_count":24,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead tr th {\n","        text-align: left;\n","    }\n","\n","    .dataframe thead tr:last-of-type th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th></th>\n","      <th colspan=\"3\" halign=\"left\">LotFrontage</th>\n","    </tr>\n","    <tr>\n","      <th></th>\n","      <th>mean</th>\n","      <th>median</th>\n","      <th>count</th>\n","    </tr>\n","    <tr>\n","      <th>Neighborhood</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Blmngtn</th>\n","      <td>46.900000</td>\n","      <td>43.0</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>Blueste</th>\n","      <td>27.300000</td>\n","      <td>24.0</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>BrDale</th>\n","      <td>21.500000</td>\n","      <td>21.0</td>\n","      <td>30</td>\n","    </tr>\n","    <tr>\n","      <th>BrkSide</th>\n","      <td>55.789474</td>\n","      <td>51.0</td>\n","      <td>95</td>\n","    </tr>\n","    <tr>\n","      <th>ClearCr</th>\n","      <td>88.150000</td>\n","      <td>80.5</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>CollgCr</th>\n","      <td>71.336364</td>\n","      <td>70.0</td>\n","      <td>220</td>\n","    </tr>\n","    <tr>\n","      <th>Crawfor</th>\n","      <td>69.951807</td>\n","      <td>70.0</td>\n","      <td>83</td>\n","    </tr>\n","    <tr>\n","      <th>Edwards</th>\n","      <td>65.153409</td>\n","      <td>64.5</td>\n","      <td>176</td>\n","    </tr>\n","    <tr>\n","      <th>Gilbert</th>\n","      <td>74.207207</td>\n","      <td>64.0</td>\n","      <td>111</td>\n","    </tr>\n","    <tr>\n","      <th>IDOTRR</th>\n","      <td>62.241379</td>\n","      <td>60.0</td>\n","      <td>87</td>\n","    </tr>\n","    <tr>\n","      <th>MeadowV</th>\n","      <td>25.606061</td>\n","      <td>21.0</td>\n","      <td>33</td>\n","    </tr>\n","    <tr>\n","      <th>Mitchel</th>\n","      <td>75.144444</td>\n","      <td>74.0</td>\n","      <td>90</td>\n","    </tr>\n","    <tr>\n","      <th>NAmes</th>\n","      <td>75.210667</td>\n","      <td>73.0</td>\n","      <td>375</td>\n","    </tr>\n","    <tr>\n","      <th>NPkVill</th>\n","      <td>28.142857</td>\n","      <td>24.0</td>\n","      <td>21</td>\n","    </tr>\n","    <tr>\n","      <th>NWAmes</th>\n","      <td>81.517647</td>\n","      <td>80.0</td>\n","      <td>85</td>\n","    </tr>\n","    <tr>\n","      <th>NoRidge</th>\n","      <td>91.629630</td>\n","      <td>89.0</td>\n","      <td>54</td>\n","    </tr>\n","    <tr>\n","      <th>NridgHt</th>\n","      <td>84.184049</td>\n","      <td>92.0</td>\n","      <td>163</td>\n","    </tr>\n","    <tr>\n","      <th>OldTown</th>\n","      <td>61.777293</td>\n","      <td>60.0</td>\n","      <td>229</td>\n","    </tr>\n","    <tr>\n","      <th>SWISU</th>\n","      <td>59.068182</td>\n","      <td>60.0</td>\n","      <td>44</td>\n","    </tr>\n","    <tr>\n","      <th>Sawyer</th>\n","      <td>74.551020</td>\n","      <td>72.0</td>\n","      <td>98</td>\n","    </tr>\n","    <tr>\n","      <th>SawyerW</th>\n","      <td>70.669811</td>\n","      <td>67.0</td>\n","      <td>106</td>\n","    </tr>\n","    <tr>\n","      <th>Somerst</th>\n","      <td>64.549383</td>\n","      <td>72.5</td>\n","      <td>162</td>\n","    </tr>\n","    <tr>\n","      <th>StoneBr</th>\n","      <td>62.173913</td>\n","      <td>60.0</td>\n","      <td>46</td>\n","    </tr>\n","    <tr>\n","      <th>Timber</th>\n","      <td>81.157895</td>\n","      <td>82.0</td>\n","      <td>57</td>\n","    </tr>\n","    <tr>\n","      <th>Veenker</th>\n","      <td>72.000000</td>\n","      <td>80.0</td>\n","      <td>16</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             LotFrontage             \n","                    mean median count\n","Neighborhood                         \n","Blmngtn        46.900000   43.0    20\n","Blueste        27.300000   24.0    10\n","BrDale         21.500000   21.0    30\n","BrkSide        55.789474   51.0    95\n","ClearCr        88.150000   80.5    20\n","CollgCr        71.336364   70.0   220\n","Crawfor        69.951807   70.0    83\n","Edwards        65.153409   64.5   176\n","Gilbert        74.207207   64.0   111\n","IDOTRR         62.241379   60.0    87\n","MeadowV        25.606061   21.0    33\n","Mitchel        75.144444   74.0    90\n","NAmes          75.210667   73.0   375\n","NPkVill        28.142857   24.0    21\n","NWAmes         81.517647   80.0    85\n","NoRidge        91.629630   89.0    54\n","NridgHt        84.184049   92.0   163\n","OldTown        61.777293   60.0   229\n","SWISU          59.068182   60.0    44\n","Sawyer         74.551020   72.0    98\n","SawyerW        70.669811   67.0   106\n","Somerst        64.549383   72.5   162\n","StoneBr        62.173913   60.0    46\n","Timber         81.157895   82.0    57\n","Veenker        72.000000   80.0    16"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["full.groupby(['Neighborhood'])[['LotFrontage']].agg(['mean','median','count'])"]},{"cell_type":"markdown","metadata":{},"source":["LotArea adalah fitur berkelanjutan jadi yang terbaik adalah menggunakan metode qcut panda untuk membaginya menjadi 10 bagian."]},{"cell_type":"code","execution_count":25,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead tr th {\n","        text-align: left;\n","    }\n","\n","    .dataframe thead tr:last-of-type th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th></th>\n","      <th colspan=\"3\" halign=\"left\">LotFrontage</th>\n","    </tr>\n","    <tr>\n","      <th></th>\n","      <th>mean</th>\n","      <th>median</th>\n","      <th>count</th>\n","    </tr>\n","    <tr>\n","      <th>LotAreaCut</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>(1299.999, 4921.8]</th>\n","      <td>35.741036</td>\n","      <td>34.0</td>\n","      <td>251</td>\n","    </tr>\n","    <tr>\n","      <th>(4921.8, 7007.2]</th>\n","      <td>55.460674</td>\n","      <td>52.0</td>\n","      <td>267</td>\n","    </tr>\n","    <tr>\n","      <th>(7007.2, 7949.0]</th>\n","      <td>62.959839</td>\n","      <td>62.0</td>\n","      <td>249</td>\n","    </tr>\n","    <tr>\n","      <th>(7949.0, 8740.4]</th>\n","      <td>67.113725</td>\n","      <td>65.0</td>\n","      <td>255</td>\n","    </tr>\n","    <tr>\n","      <th>(8740.4, 9452.0]</th>\n","      <td>69.959184</td>\n","      <td>70.0</td>\n","      <td>245</td>\n","    </tr>\n","    <tr>\n","      <th>(9452.0, 10148.8]</th>\n","      <td>73.988235</td>\n","      <td>75.0</td>\n","      <td>255</td>\n","    </tr>\n","    <tr>\n","      <th>(10148.8, 11000.0]</th>\n","      <td>73.636364</td>\n","      <td>75.0</td>\n","      <td>253</td>\n","    </tr>\n","    <tr>\n","      <th>(11000.0, 12196.8]</th>\n","      <td>83.371681</td>\n","      <td>82.0</td>\n","      <td>226</td>\n","    </tr>\n","    <tr>\n","      <th>(12196.8, 14285.8]</th>\n","      <td>84.973684</td>\n","      <td>85.0</td>\n","      <td>228</td>\n","    </tr>\n","    <tr>\n","      <th>(14285.8, 215245.0]</th>\n","      <td>92.846535</td>\n","      <td>90.0</td>\n","      <td>202</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                    LotFrontage             \n","                           mean median count\n","LotAreaCut                                  \n","(1299.999, 4921.8]    35.741036   34.0   251\n","(4921.8, 7007.2]      55.460674   52.0   267\n","(7007.2, 7949.0]      62.959839   62.0   249\n","(7949.0, 8740.4]      67.113725   65.0   255\n","(8740.4, 9452.0]      69.959184   70.0   245\n","(9452.0, 10148.8]     73.988235   75.0   255\n","(10148.8, 11000.0]    73.636364   75.0   253\n","(11000.0, 12196.8]    83.371681   82.0   226\n","(12196.8, 14285.8]    84.973684   85.0   228\n","(14285.8, 215245.0]   92.846535   90.0   202"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["full['LotAreaCut'] = pd.qcut(full.LotArea,10)\n","\n","full.groupby([full['LotAreaCut']])[['LotFrontage']].agg(['mean','median','count'])"]},{"cell_type":"markdown","metadata":{},"source":["Jadi mari kita hitung nilai yang hilang dari LotFrontage seperti yang dinyatakan di atas dengan median LotArea dan Neighborhood."]},{"cell_type":"code","execution_count":26,"metadata":{"trusted":true},"outputs":[],"source":["full['LotFrontage']= full.groupby(['LotAreaCut','Neighborhood'])['LotFrontage'].transform(lambda x : x.fillna(x.median()))\n","full['LotFrontage']= full.groupby(['LotAreaCut'])['LotFrontage'].transform(lambda x : x.fillna(x.median()))"]},{"cell_type":"markdown","metadata":{},"source":["Sekarang mari kita periksa kembali nilai yang hilang untuk melihat nilai yang hilang dari LotFrontage berhasil diperhitungkan."]},{"cell_type":"code","execution_count":27,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["PoolQC          2908\n","MiscFeature     2812\n","Alley           2719\n","Fence           2346\n","SalePrice       1459\n","FireplaceQu     1420\n","GarageCond       159\n","GarageQual       159\n","GarageFinish     159\n","GarageYrBlt      159\n","GarageType       157\n","BsmtCond          82\n","BsmtExposure      82\n","BsmtQual          81\n","BsmtFinType2      80\n","BsmtFinType1      79\n","MasVnrType        24\n","MasVnrArea        23\n","MSZoning           4\n","Functional         2\n","BsmtHalfBath       2\n","Utilities          2\n","BsmtFullBath       2\n","KitchenQual        1\n","Electrical         1\n","TotalBsmtSF        1\n","BsmtUnfSF          1\n","GarageCars         1\n","GarageArea         1\n","BsmtFinSF2         1\n","BsmtFinSF1         1\n","Exterior2nd        1\n","Exterior1st        1\n","SaleType           1\n","dtype: int64"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["missing_values = full.isnull().sum()\n","\n","missing_values[missing_values>0].sort_values(ascending = False)"]},{"cell_type":"markdown","metadata":{},"source":["Sekarang mari kita fokus pada fitur numerik dengan satu nilai yang hilang dan menggantinya dengan 0"]},{"cell_type":"code","execution_count":28,"metadata":{"trusted":true},"outputs":[],"source":["columns = [\"MasVnrArea\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"GarageCars\", \"BsmtFinSF2\", \"BsmtFinSF1\", \"GarageArea\"]\n","for col in columns:full[col].fillna(0,inplace= True)"]},{"cell_type":"markdown","metadata":{},"source":["Sekarang mari kita fokus pada beberapa fitur kategoris dengan jumlah nilai yang hilang dan menggantinya dengan 'Tidak Ada'"]},{"cell_type":"code","execution_count":29,"metadata":{"trusted":true},"outputs":[],"source":["columns1 = [\"PoolQC\" , \"MiscFeature\", \"Alley\", \"Fence\", \"FireplaceQu\", \"GarageQual\", \"GarageCond\", \"GarageFinish\",\n","\"GarageYrBlt\", \"GarageType\", \"BsmtExposure\", \"BsmtCond\", \"BsmtQual\", \"BsmtFinType2\", \"BsmtFinType1\", \"MasVnrType\"]\n","for col1 in columns1:full[col1].fillna('None',inplace = True)"]},{"cell_type":"markdown","metadata":{},"source":["Sekarang mari kita fokus pada beberapa fitur kategoris dengan lebih sedikit nilai yang hilang dan menggantinya dengan nilai yang paling sering muncul yang merupakan mode dari fitur tersebut."]},{"cell_type":"code","execution_count":30,"metadata":{"trusted":true},"outputs":[],"source":["columns2 = [\"MSZoning\", \"BsmtFullBath\", \"BsmtHalfBath\", \"Utilities\", \"Functional\",\n","            \"Electrical\", \"KitchenQual\", \"SaleType\",\"Exterior1st\", \"Exterior2nd\"]\n","\n","for col2 in columns2:\n","    full[col2].fillna(full[col2].mode()[0],inplace = True)"]},{"cell_type":"markdown","metadata":{},"source":["Sekarang mari kita periksa apakah kita memiliki nilai lain yang hilang yang perlu diperhitungkan kecuali Harga Jual untuk dataset uji yang merupakan variabel target yang akan ditentukan."]},{"cell_type":"code","execution_count":31,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["SalePrice    1459\n","dtype: int64"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["full.isnull().sum()[full.isnull().sum()>0]"]},{"cell_type":"code","execution_count":32,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["Index(['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond',\n","       'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2',\n","       'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n","       'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath',\n","       'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces',\n","       'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF',\n","       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal',\n","       'MoSold', 'YrSold', 'SalePrice'],\n","      dtype='object')"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["numeric_features = full.select_dtypes(include=[np.number])\n","numeric_features.columns"]},{"cell_type":"code","execution_count":33,"metadata":{"trusted":true},"outputs":[],"source":["Numstr = [\"MSSubClass\",\"BsmtFullBath\",\"BsmtHalfBath\",\"HalfBath\",\"BedroomAbvGr\",\"KitchenAbvGr\",\"MoSold\",\n","          \"YrSold\",\"YearBuilt\",\"YearRemodAdd\",\"LowQualFinSF\",\"GarageYrBlt\"]\n","\n","for i in Numstr:\n","    full[i]=full[i].astype(str)"]},{"cell_type":"code","execution_count":34,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead tr th {\n","        text-align: left;\n","    }\n","\n","    .dataframe thead tr:last-of-type th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th></th>\n","      <th colspan=\"3\" halign=\"left\">SalePrice</th>\n","    </tr>\n","    <tr>\n","      <th></th>\n","      <th>mean</th>\n","      <th>median</th>\n","      <th>count</th>\n","    </tr>\n","    <tr>\n","      <th>MSSubClass</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>120</th>\n","      <td>200779.080460</td>\n","      <td>192000.0</td>\n","      <td>87</td>\n","    </tr>\n","    <tr>\n","      <th>150</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>160</th>\n","      <td>138647.380952</td>\n","      <td>146000.0</td>\n","      <td>63</td>\n","    </tr>\n","    <tr>\n","      <th>180</th>\n","      <td>102300.000000</td>\n","      <td>88500.0</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>190</th>\n","      <td>129613.333333</td>\n","      <td>128250.0</td>\n","      <td>30</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>185224.811567</td>\n","      <td>159250.0</td>\n","      <td>536</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>95829.724638</td>\n","      <td>99900.0</td>\n","      <td>69</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>156125.000000</td>\n","      <td>142500.0</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>45</th>\n","      <td>108591.666667</td>\n","      <td>107500.0</td>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>50</th>\n","      <td>143302.972222</td>\n","      <td>132000.0</td>\n","      <td>144</td>\n","    </tr>\n","    <tr>\n","      <th>60</th>\n","      <td>240403.542088</td>\n","      <td>216000.0</td>\n","      <td>297</td>\n","    </tr>\n","    <tr>\n","      <th>70</th>\n","      <td>166772.416667</td>\n","      <td>156000.0</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>75</th>\n","      <td>192437.500000</td>\n","      <td>163500.0</td>\n","      <td>16</td>\n","    </tr>\n","    <tr>\n","      <th>80</th>\n","      <td>169736.551724</td>\n","      <td>166500.0</td>\n","      <td>58</td>\n","    </tr>\n","    <tr>\n","      <th>85</th>\n","      <td>147810.000000</td>\n","      <td>140750.0</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>90</th>\n","      <td>133541.076923</td>\n","      <td>135980.0</td>\n","      <td>52</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                SalePrice                \n","                     mean    median count\n","MSSubClass                               \n","120         200779.080460  192000.0    87\n","150                   NaN       NaN     0\n","160         138647.380952  146000.0    63\n","180         102300.000000   88500.0    10\n","190         129613.333333  128250.0    30\n","20          185224.811567  159250.0   536\n","30           95829.724638   99900.0    69\n","40          156125.000000  142500.0     4\n","45          108591.666667  107500.0    12\n","50          143302.972222  132000.0   144\n","60          240403.542088  216000.0   297\n","70          166772.416667  156000.0    60\n","75          192437.500000  163500.0    16\n","80          169736.551724  166500.0    58\n","85          147810.000000  140750.0    20\n","90          133541.076923  135980.0    52"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["full.groupby(['MSSubClass'])[['SalePrice']].agg(['mean','median','count'])"]},{"cell_type":"code","execution_count":35,"metadata":{"trusted":true},"outputs":[],"source":["def map_values():\n","    full[\"oMSSubClass\"] = full.MSSubClass.map({'180':1, \n","                                        '30':2, '45':2, \n","                                        '190':3, '50':3, '90':3, \n","                                        '85':4, '40':4, '160':4, \n","                                        '70':5, '20':5, '75':5, '80':5, '150':5,\n","                                        '120': 6, '60':6})\n","    \n","    full[\"oMSZoning\"] = full.MSZoning.map({'C (all)':1, 'RH':2, 'RM':2, 'RL':3, 'FV':4})\n","    full[\"oNeighborhood\"] = full.Neighborhood.map({'MeadowV':1,\n","                                               'IDOTRR':2, 'BrDale':2,\n","                                               'OldTown':3, 'Edwards':3, 'BrkSide':3,\n","                                               'Sawyer':4, 'Blueste':4, 'SWISU':4, 'NAmes':4,\n","                                               'NPkVill':5, 'Mitchel':5,\n","                                               'SawyerW':6, 'Gilbert':6, 'NWAmes':6,\n","                                               'Blmngtn':7, 'CollgCr':7, 'ClearCr':7, 'Crawfor':7,\n","                                               'Veenker':8, 'Somerst':8, 'Timber':8,\n","                                               'StoneBr':9,\n","                                               'NoRidge':10, 'NridgHt':10})\n","    \n","    full[\"oCondition1\"] = full.Condition1.map({'Artery':1,\n","                                           'Feedr':2, 'RRAe':2,\n","                                           'Norm':3, 'RRAn':3,\n","                                           'PosN':4, 'RRNe':4,\n","                                           'PosA':5 ,'RRNn':5})\n","    \n","    full[\"oBldgType\"] = full.BldgType.map({'2fmCon':1, 'Duplex':1, 'Twnhs':1, '1Fam':2, 'TwnhsE':2})\n","    \n","    full[\"oHouseStyle\"] = full.HouseStyle.map({'1.5Unf':1, \n","                                           '1.5Fin':2, '2.5Unf':2, 'SFoyer':2, \n","                                           '1Story':3, 'SLvl':3,\n","                                           '2Story':4, '2.5Fin':4})\n","    \n","    full[\"oExterior1st\"] = full.Exterior1st.map({'BrkComm':1,\n","                                             'AsphShn':2, 'CBlock':2, 'AsbShng':2,\n","                                             'WdShing':3, 'Wd Sdng':3, 'MetalSd':3, 'Stucco':3, 'HdBoard':3,\n","                                             'BrkFace':4, 'Plywood':4,\n","                                             'VinylSd':5,\n","                                             'CemntBd':6,\n","                                             'Stone':7, 'ImStucc':7})\n","    \n","    full[\"oMasVnrType\"] = full.MasVnrType.map({'BrkCmn':1, 'None':1, 'BrkFace':2, 'Stone':3})\n","    \n","    full[\"oExterQual\"] = full.ExterQual.map({'Fa':1, 'TA':2, 'Gd':3, 'Ex':4})\n","    \n","    full[\"oFoundation\"] = full.Foundation.map({'Slab':1, \n","                                           'BrkTil':2, 'CBlock':2, 'Stone':2,\n","                                           'Wood':3, 'PConc':4})\n","    \n","    full[\"oBsmtQual\"] = full.BsmtQual.map({'Fa':2, 'None':1, 'TA':3, 'Gd':4, 'Ex':5})\n","    \n","    full[\"oBsmtExposure\"] = full.BsmtExposure.map({'None':1, 'No':2, 'Av':3, 'Mn':3, 'Gd':4})\n","    \n","    full[\"oHeating\"] = full.Heating.map({'Floor':1, 'Grav':1, 'Wall':2, 'OthW':3, 'GasW':4, 'GasA':5})\n","    \n","    full[\"oHeatingQC\"] = full.HeatingQC.map({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\n","    \n","    full[\"oKitchenQual\"] = full.KitchenQual.map({'Fa':1, 'TA':2, 'Gd':3, 'Ex':4})\n","    \n","    full[\"oFunctional\"] = full.Functional.map({'Maj2':1, 'Maj1':2, 'Min1':2, 'Min2':2, 'Mod':2, 'Sev':2, 'Typ':3})\n","    \n","    full[\"oFireplaceQu\"] = full.FireplaceQu.map({'None':1, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\n","    \n","    full[\"oGarageType\"] = full.GarageType.map({'CarPort':1, 'None':1,\n","                                           'Detchd':2,\n","                                           '2Types':3, 'Basment':3,\n","                                           'Attchd':4, 'BuiltIn':5})\n","    \n","    full[\"oGarageFinish\"] = full.GarageFinish.map({'None':1, 'Unf':2, 'RFn':3, 'Fin':4})\n","    \n","    full[\"oPavedDrive\"] = full.PavedDrive.map({'N':1, 'P':2, 'Y':3})\n","    \n","    full[\"oSaleType\"] = full.SaleType.map({'COD':1, 'ConLD':1, 'ConLI':1, 'ConLw':1, 'Oth':1, 'WD':1,\n","                                       'CWD':2, 'Con':3, 'New':3})\n","    \n","    full[\"oSaleCondition\"] = full.SaleCondition.map({'AdjLand':1, 'Abnorml':2, 'Alloca':2, 'Family':2, 'Normal':3, 'Partial':4})            \n","                \n","                        \n","                        \n","    \n","    return \"Done!\"\n"]},{"cell_type":"code","execution_count":36,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["'Done!'"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["map_values()"]},{"cell_type":"code","execution_count":37,"metadata":{"trusted":true},"outputs":[],"source":["# drop two unwanted columns\n","full.drop(\"LotAreaCut\",axis=1,inplace=True)\n","\n","full.drop(['SalePrice'],axis=1,inplace=True)"]},{"cell_type":"code","execution_count":38,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>YearBuilt</th>\n","      <th>YearRemodAdd</th>\n","      <th>GarageYrBlt</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2003</td>\n","      <td>2003</td>\n","      <td>2003.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1976</td>\n","      <td>1976</td>\n","      <td>1976.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2001</td>\n","      <td>2002</td>\n","      <td>2001.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1915</td>\n","      <td>1970</td>\n","      <td>1998.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2000</td>\n","      <td>2000</td>\n","      <td>2000.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  YearBuilt YearRemodAdd GarageYrBlt\n","0      2003         2003      2003.0\n","1      1976         1976      1976.0\n","2      2001         2002      2001.0\n","3      1915         1970      1998.0\n","4      2000         2000      2000.0"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["full[['YearBuilt','YearRemodAdd','GarageYrBlt']].head()"]},{"cell_type":"markdown","metadata":{},"source":["Let us create a class for the LabelEncoder to fit and transform some of the identified features"]},{"cell_type":"code","execution_count":39,"metadata":{"trusted":true},"outputs":[],"source":["class labenc(BaseEstimator,TransformerMixin):\n","    def __init__(self):\n","        pass\n","    def fit(self,X,y=None):\n","        return self\n","    def transform(self,X):\n","        label = LabelEncoder()\n","        X['YearBuilt']=label.fit_transform(X['YearBuilt'])\n","        X['YearRemodAdd']=label.fit_transform(X['YearRemodAdd'])\n","        X['GarageYrBlt']=label.fit_transform(X['GarageYrBlt'])\n","        return X\n","        "]},{"cell_type":"code","execution_count":40,"metadata":{"trusted":true},"outputs":[],"source":["class skewness(BaseEstimator,TransformerMixin):\n","    def __init__(self,skew=0.5):\n","        self.skew = skew\n","    def fit(self,X,y=None):\n","        return self\n","    def transform(self,X):\n","        X_numeric=X.select_dtypes(exclude=[\"object\"])\n","        skewness = X_numeric.apply(lambda x: skew(x))\n","        skewness_features = skewness[abs(skewness) >= self.skew].index\n","        X[skewness_features] = np.log1p(X[skewness_features])\n","        return X"]},{"cell_type":"code","execution_count":41,"metadata":{"trusted":true},"outputs":[],"source":["class dummies(BaseEstimator,TransformerMixin):\n","    def __init__(self):\n","        pass\n","    def fit(self,X,y=None):\n","        return self\n","    def transform(self,X):\n","        X = pd.get_dummies(X)\n","        return X"]},{"cell_type":"markdown","metadata":{},"source":["Sekarang kita akan menggunakan pipeline untuk menghubungkan beberapa estimator menjadi satu. Ini berguna karena seringkali ada urutan langkah yang tetap dalam pemrosesan data, misalnya pemilihan fitur, normalisasi, dan klasifikasi. Pipeline melayani dua tujuan di sini:\n","\n","Kenyamanan: Anda hanya perlu memanggil fit dan memprediksi sekali pada data Anda agar sesuai dengan seluruh urutan estimator.\n","Pemilihan parameter gabungan: Anda dapat menelusuri parameter semua estimator dalam pipeline sekaligus.\n","Semua estimator dalam pipa, kecuali yang terakhir, harus transformator (yaitu harus memiliki metode transformasi). Penaksir terakhir dapat berupa jenis apa pun (transformator, pengklasifikasi, dll.)."]},{"cell_type":"code","execution_count":42,"metadata":{"trusted":true},"outputs":[],"source":["pipeline = Pipeline([('labenc',labenc()),('skewness',skewness(skew =1)),('dummies',dummies())])"]},{"cell_type":"code","execution_count":43,"metadata":{"trusted":true},"outputs":[],"source":["full_copy = full.copy()\n","data_pipeline = pipeline.fit_transform(full_copy)"]},{"cell_type":"code","execution_count":44,"metadata":{"trusted":true},"outputs":[],"source":["robust_scaler = RobustScaler()"]},{"cell_type":"code","execution_count":45,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["1458"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["n_train = train.shape[0]\n","n_train"]},{"cell_type":"code","execution_count":46,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["((1458, 405), (1458,), (1459, 405))"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["X= data_pipeline[:n_train]\n","y = train.SalePrice\n","test_X = data_pipeline[n_train:]\n","X.shape,y.shape,test_X.shape"]},{"cell_type":"code","execution_count":47,"metadata":{"trusted":true},"outputs":[],"source":["X_scaled = robust_scaler.fit(X).transform(X)\n","y_log = np.log(train.SalePrice)\n","test_X_scaled = robust_scaler.transform(test_X)"]},{"cell_type":"code","execution_count":48,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["((1458, 405), (1458,), (1459, 405))"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["X_scaled.shape,y_log.shape,test_X.shape"]},{"cell_type":"markdown","metadata":{},"source":["Now we will perform some feature selection like Lasso "]},{"cell_type":"code","execution_count":49,"metadata":{"trusted":true},"outputs":[],"source":["class add_feature(BaseEstimator, TransformerMixin):\n","    def __init__(self,additional=1):\n","        self.additional = additional\n","    \n","    def fit(self,X,y=None):\n","        return self\n","    \n","    def transform(self,X):\n","        if self.additional==1:\n","            X[\"TotalHouse\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"]   \n","            X[\"TotalArea\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"] + X[\"GarageArea\"]\n","            \n","        else:\n","            X[\"TotalHouse\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"]   \n","            X[\"TotalArea\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"] + X[\"GarageArea\"]\n","            \n","            X[\"+_TotalHouse_OverallQual\"] = X[\"TotalHouse\"] * X[\"OverallQual\"]\n","            X[\"+_GrLivArea_OverallQual\"] = X[\"GrLivArea\"] * X[\"OverallQual\"]\n","            X[\"+_oMSZoning_TotalHouse\"] = X[\"oMSZoning\"] * X[\"TotalHouse\"]\n","            X[\"+_oMSZoning_OverallQual\"] = X[\"oMSZoning\"] + X[\"OverallQual\"]\n","            X[\"+_oMSZoning_YearBuilt\"] = X[\"oMSZoning\"] + X[\"YearBuilt\"]\n","            X[\"+_oNeighborhood_TotalHouse\"] = X[\"oNeighborhood\"] * X[\"TotalHouse\"]\n","            X[\"+_oNeighborhood_OverallQual\"] = X[\"oNeighborhood\"] + X[\"OverallQual\"]\n","            X[\"+_oNeighborhood_YearBuilt\"] = X[\"oNeighborhood\"] + X[\"YearBuilt\"]\n","            X[\"+_BsmtFinSF1_OverallQual\"] = X[\"BsmtFinSF1\"] * X[\"OverallQual\"]\n","            \n","            X[\"-_oFunctional_TotalHouse\"] = X[\"oFunctional\"] * X[\"TotalHouse\"]\n","            X[\"-_oFunctional_OverallQual\"] = X[\"oFunctional\"] + X[\"OverallQual\"]\n","            X[\"-_LotArea_OverallQual\"] = X[\"LotArea\"] * X[\"OverallQual\"]\n","            X[\"-_TotalHouse_LotArea\"] = X[\"TotalHouse\"] + X[\"LotArea\"]\n","            X[\"-_oCondition1_TotalHouse\"] = X[\"oCondition1\"] * X[\"TotalHouse\"]\n","            X[\"-_oCondition1_OverallQual\"] = X[\"oCondition1\"] + X[\"OverallQual\"]\n","            \n","           \n","            X[\"Bsmt\"] = X[\"BsmtFinSF1\"] + X[\"BsmtFinSF2\"] + X[\"BsmtUnfSF\"]\n","            X[\"Rooms\"] = X[\"FullBath\"]+X[\"TotRmsAbvGrd\"]\n","            X[\"PorchArea\"] = X[\"OpenPorchSF\"]+X[\"EnclosedPorch\"]+X[\"3SsnPorch\"]+X[\"ScreenPorch\"]\n","            X[\"TotalPlace\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"] + X[\"GarageArea\"] + X[\"OpenPorchSF\"]+X[\"EnclosedPorch\"]+X[\"3SsnPorch\"]+X[\"ScreenPorch\"]\n","\n","    \n","            return X"]},{"cell_type":"code","execution_count":50,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["(2917, 426)"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["pipeline = Pipeline([('labenc',labenc()),('add_feature', add_feature(additional=2)),\n","                     ('skewness',skewness(skew =1)),('dummies',dummies())])\n","\n","full_pipe = pipeline.fit_transform(full)\n","full_pipe.shape"]},{"cell_type":"code","execution_count":51,"metadata":{"trusted":true},"outputs":[],"source":["n_train=train.shape[0]\n","X = full_pipe[:n_train]\n","test_X = full_pipe[n_train:]\n","y= train.SalePrice\n","\n","X_scaled = robust_scaler.fit(X).transform(X)\n","y_log = np.log(train.SalePrice)\n","test_X_scaled = robust_scaler.transform(test_X)"]},{"cell_type":"code","execution_count":52,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(1458, 426)\n"]}],"source":["print(X_scaled.shape)"]},{"cell_type":"code","execution_count":53,"metadata":{"trusted":true},"outputs":[],"source":["# Now let us define Root Mean Square Error \n","def rmse_cv(model,X,y):\n","    rmse = np.sqrt(-cross_val_score(model,X,y,scoring=\"neg_mean_squared_error\",cv=5))\n","    return rmse"]},{"cell_type":"markdown","metadata":{},"source":["We choose 4 models and use 5-folds cross-calidation to evaluate these models.\n","\n","### Models include:\n","\n","   - LinearRegression\n","   - Ridge\n","   - Lasso\n","   - Random Forest"]},{"cell_type":"code","execution_count":54,"metadata":{"trusted":true},"outputs":[],"source":["models = [LinearRegression(),\n","             Ridge(),\n","             Lasso(alpha=0.01,max_iter=10000),\n","             RandomForestRegressor(),\n","             GradientBoostingRegressor(),\n","             SVR(),\n","             LinearSVR(),\n","             ElasticNet(alpha = 0.001,max_iter=10000),\n","             SGDRegressor(max_iter=1000, tol = 1e-3),\n","             BayesianRidge(),\n","             KernelRidge(alpha=0.6,kernel='polynomial',degree = 2,coef0=2.5),\n","             ExtraTreesRegressor(),\n","             XGBRegressor()\n","             ]"]},{"cell_type":"code","execution_count":55,"metadata":{"trusted":true},"outputs":[],"source":["names = ['LR','Ridge','Lasso','RF','GBR','SVR','LSVR','ENet','SGDR','BayRidge','Kernel','XTreeR','XGBR']"]},{"cell_type":"code","execution_count":56,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["LR: 563380228.110560, 281367281.934851\n","Ridge: 0.117596, 0.009054\n","Lasso: 0.120932, 0.005813\n","RF: 0.131034, 0.007196\n","GBR: 0.121335, 0.004460\n","SVR: 0.131747, 0.010868\n","LSVR: 0.126264, 0.007887\n","ENet: 0.108729, 0.005422\n","SGDR: 0.300234, 0.011501\n","BayRidge: 0.110576, 0.005997\n","Kernel: 0.109421, 0.005545\n","XTreeR: 0.125309, 0.007176\n","XGBR: 0.129406, 0.007979\n"]}],"source":["for model,name in zip(models,names):\n","    score = rmse_cv(model,X_scaled,y_log)\n","    print(\"{}: {:.6f}, {:4f}\".format(name,score.mean(),score.std()))"]},{"cell_type":"code","execution_count":57,"metadata":{"trusted":true},"outputs":[],"source":["# To define the average weight \n","class AverageWeight(BaseEstimator, RegressorMixin):\n","    def __init__(self,model,weight):\n","        self.model = model\n","        self.weight = weight\n","        \n","    def fit(self,X,y):\n","        self.models_ = [clone(x) for x in self.model]\n","        for model in self.models_:\n","            model.fit(X,y)\n","        return self\n","    \n","    def predict(self,X):\n","        w = list()\n","        pred = np.array([model.predict(X) for model in self.models_])\n","        # for every data point, single model prediction times weight, then add them together\n","        for data in range(pred.shape[1]):\n","            single = [pred[model,data]*weight for model,weight in zip(range(pred.shape[0]),self.weight)]\n","            w.append(np.sum(single))\n","        return w"]},{"cell_type":"code","execution_count":58,"metadata":{"trusted":true},"outputs":[],"source":["lasso = Lasso(alpha= 0.0005, max_iter= 10000)\n","ridge = Ridge(alpha=45, max_iter= 10000)\n","svr = SVR(C = 0.2, epsilon= 0.025, gamma = 0.0004, kernel = 'rbf')\n","ker = KernelRidge(alpha=0.15 ,kernel='polynomial',degree=3 , coef0=0.9)\n","ela = ElasticNet(alpha=0.0065,l1_ratio=0.075,max_iter=10000)\n","bay = BayesianRidge()"]},{"cell_type":"markdown","metadata":{},"source":["#### Finally to calculate the average weights let us look at the following code"]},{"cell_type":"code","execution_count":59,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0.1092439889607516\n"]}],"source":["# Assign weights to all the above 6 models\n","w1 = 0.047\n","w2 = 0.2\n","w3 = 0.25\n","w4 = 0.3\n","w5 = 0.003\n","w6 = 0.2\n","\n","weight_avg = AverageWeight(model = [lasso,ridge,svr,ker,ela,bay],weight=[w1,w2,w3,w4,w5,w6])\n","score = rmse_cv(weight_avg,X_scaled,y_log)\n","print(score.mean())"]},{"cell_type":"markdown","metadata":{},"source":["If we consider only two models then the score will vary"]},{"cell_type":"code","execution_count":60,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0.11166439558199041\n"]}],"source":["weight_avg = AverageWeight(model = [svr,ker],weight=[0.50,0.50])\n","score = rmse_cv(weight_avg,X_scaled,y_log)\n","print(score.mean())"]},{"cell_type":"markdown","metadata":{},"source":["Jadi secara ringkas, Rata-rata tertimbang adalah versi rata-rata sederhana yang sedikit dimodifikasi, di mana prediksi setiap model dikalikan dengan bobot dan kemudian rata-ratanya dihitung.\n","\n","## 2.4 Stacking <a id=\"2.4\"></a> <br>\n","![](https://d1m75rqqgidzqn.cloudfront.net/wp-data/2020/05/21160015/shutterstock_1159836664-696x464.jpg)\n","Stacking adalah teknik pembelajaran ensemble yang menggunakan prediksi dari beberapa model (misalnya pohon keputusan, knn atau svm) untuk membangun model baru. Model ini digunakan untuk membuat prediksi pada test set.\n","\n","Stacking, juga dikenal sebagai Stacked Generalization adalah teknik ensemble yang menggabungkan beberapa klasifikasi atau model regresi melalui meta-classifier atau meta-regressor. Model tingkat dasar dilatih pada set pelatihan yang lengkap, kemudian model meta dilatih pada fitur yang merupakan keluaran dari model tingkat dasar. Tingkat dasar sering terdiri dari algoritma pembelajaran yang berbeda dan oleh karena itu susun ensembel sering kali heterogen. Berikut adalah diagram yang menggambarkan proses\n","\n","Di bawah ini adalah penjelasan langkah demi langkah untuk ensemble bertumpuk sederhana:\n","\n","![](https://www.researchgate.net/publication/324552457/figure/fig3/AS:616245728645121@1523935839872/An-example-scheme-of-stacking-ensemble-learning.png)\n","\n","Step 1:The train set is split into 10 parts.\n","![](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/05/image-11-300x217.png)\n","Step 2:A base model (suppose a decision tree) is fitted on 9 parts and predictions are made for the 10th part. This is done for each part of the train set.\n","![](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/05/image-10-300x249.png)\n","Step 3:The base model (in this case, decision tree) is then fitted on the whole train dataset.\n","\n","Step 4:Using this model, predictions are made on the test set.\n","![](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/05/image-2-300x225.png)\n","Step 5:Steps 2 to 4 are repeated for another base model (say knn) resulting in another set of predictions for the train set and test set.\n","![](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/05/image-3-300x224.png)\n","Step 6:The predictions from the train set are used as features to build a new model.\n","![](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/05/image12-292x300.png)\n","\n","Step 7:This model is used to make final predictions on the test prediction set.\n","\n","\n","In order to simplify the above explanation, the stacking model we have created has only two levels. The decision tree and knn models are built at level zero, while a logistic regression model is built at level one. Feel free to create multiple levels in a stacking model.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class stacking(BaseEstimator, RegressorMixin, TransformerMixin):\n","    def __init__(self,mod,meta_model):\n","        self.mod = mod\n","        self.meta_model = meta_model\n","        self.kf = KFold(n_splits=5, random_state=42, shuffle=True)\n","        \n","    def fit(self,X,y):\n","        self.saved_model = [list() for i in self.mod]\n","        oof_train = np.zeros((X.shape[0], len(self.mod)))\n","        \n","        for i,model in enumerate(self.mod):\n","            for train_index, val_index in self.kf.split(X,y):\n","                renew_model = clone(model)\n","                renew_model.fit(X[train_index], y[train_index])\n","                self.saved_model[i].append(renew_model)\n","                oof_train[val_index,i] = renew_model.predict(X[val_index])\n","        \n","        self.meta_model.fit(oof_train,y)\n","        return self\n","    \n","    def predict(self,X):\n","        whole_test = np.column_stack([np.column_stack(model.predict(X) for model in single_model).mean(axis=1) \n","                                      for single_model in self.saved_model]) \n","        return self.meta_model.predict(whole_test)\n","    \n","    def get_oof(self,X,y,test_X):\n","        oof = np.zeros((X.shape[0],len(self.mod)))\n","        test_single = np.zeros((test_X.shape[0],5))\n","        test_mean = np.zeros((test_X.shape[0],len(self.mod)))\n","        for i,model in enumerate(self.mod):\n","            for j, (train_index,val_index) in enumerate(self.kf.split(X,y)):\n","                clone_model = clone(model)\n","                clone_model.fit(X[train_index],y[train_index])\n","                oof[val_index,i] = clone_model.predict(X[val_index])\n","                test_single[:,j] = clone_model.predict(test_X)\n","            test_mean[:,i] = test_single.mean(axis=1)\n","        return oof, test_mean"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.impute import SimpleImputer"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["X_scaled_imputed = SimpleImputer().fit_transform(X_scaled)\n","y_log_imputed = SimpleImputer().fit_transform(y_log.values.reshape(-1,1)).ravel()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["stack_model = stacking(mod=[lasso,ridge,svr,ker,ela,bay],meta_model=ker)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["score = rmse_cv(stack_model,X_scaled_imputed,y_log_imputed)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0.10770838136721914\n"]}],"source":["print(score.mean())"]},{"cell_type":"markdown","metadata":{},"source":["## 2.5 Blending <a id=\"2.5\"></a> <br>\n","\n","Blending follows the same approach as stacking but uses only a holdout (validation) set from the train set to make predictions. In other words, unlike stacking, the predictions are made on the holdout set only. The holdout set and the predictions are used to build a model which is run on the test set. Here is a detailed explanation of the blending process:\n","\n","Step 1: The train set is split into training and validation sets\n","![](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/05/image-7-300x226.png)\n","\n","Step 2: Model(s) are fitted on the training set.\n","![](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/05/image-5-300x228.png)\n","\n","Step 3: The predictions are made on the validation set and the test set.\n","\n","Step 4: The validation set and its predictions are used as features to build a new model.\n","\n","Step 5: This model is used to make final predictions on the test and meta-features.\n","\n","\n","The difference between stacking and blending is that Stacking uses out-of-fold predictions for the train set of the next layer (i.e meta-model), and Blending uses a validation set (let’s say, 10-15% of the training set) to train the next layer.\n","\n","We’ll build two models, decision tree and knn, on the train set in order to make predictions on the validation set.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.datasets import load_wine\n","# define dataset\n","X,y = load_wine().data,load_wine().target"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=1)\n","X_train, X_val, y_train, y_val = train_test_split(\n","    X_train, y_train, test_size=0.25, random_state=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["x_val=pd.DataFrame(X_val)\n","x_test=pd.DataFrame(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model1 = DecisionTreeClassifier()\n","model1.fit(X_train, y_train)\n","val_pred1=model1.predict(X_val)\n","test_pred1=model1.predict(X_test)\n","val_pred1=pd.DataFrame(val_pred1)\n","test_pred1=pd.DataFrame(test_pred1)\n","\n","model2 = KNeighborsClassifier()\n","model2.fit(X_train,y_train)\n","val_pred2=model2.predict(X_val)\n","test_pred2=model2.predict(X_test)\n","val_pred2=pd.DataFrame(val_pred2)\n","test_pred2=pd.DataFrame(test_pred2)"]},{"cell_type":"markdown","metadata":{},"source":["Combining the meta-features and the validation set, a logistic regression model is built to make predictions on the test set."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["0.9166666666666666"]},"execution_count":63,"metadata":{},"output_type":"execute_result"}],"source":["df_val=pd.concat([x_val, val_pred1,val_pred2],axis=1)\n","df_test=pd.concat([x_test, test_pred1,test_pred2],axis=1)\n","\n","model = LogisticRegression()\n","model.fit(df_val,y_val)\n","model.score(df_test,y_test)"]},{"cell_type":"markdown","metadata":{},"source":["## 2.6 Bagging <a id=\"2.6\"></a> <br>\n","![](https://miro.medium.com/max/700/1*DFHUbdz6EyOuMYP4pDnFlw.jpeg)\n","\n","**Bagging**, adalah singkatan dari kombinasi bootstrap dan agregasi. Bootstrapping adalah metode untuk membantu mengurangi varians dari classifier dan mengurangi overfitting, dengan melakukan resampling data dari training set dengan kardinalitas yang sama dengan set aslinya. Model yang dibuat harus kurang overfitted daripada model individu tunggal.\n","\n","Varians yang tinggi untuk suatu model tidak baik, menunjukkan bahwa kinerjanya sensitif terhadap data pelatihan yang diberikan. Jadi, bahkan jika lebih banyak data pelatihan diberikan, model mungkin masih berkinerja buruk. Dan, bahkan mungkin tidak mengurangi varians model kami.\n","\n","Bagging adalah metode yang efektif ketika Anda memiliki data yang terbatas, dan dengan menggunakan sampel, Anda bisa mendapatkan perkiraan dengan menggabungkan skor pada banyak sampel.\n","\n","Pendekatan paling sederhana dengan bagging adalah dengan menggunakan beberapa sub-sampel kecil dan mengantonginya, jika akurasi ensemble jauh lebih tinggi daripada model dasar, itu berhasil; jika tidak, gunakan subsampel yang lebih besar. Menggunakan subsampel yang lebih besar tidak dijamin akan meningkatkan hasil Anda. Dalam bagging ada tradeoff antara akurasi model dasar dan keuntungan yang Anda dapatkan melalui bagging. Agregasi dari bagging dapat meningkatkan ansambel secara signifikan ketika Anda memiliki model yang tidak stabil, namun ketika model dasar Anda lebih stabil — dilatih pada subsampel yang lebih besar dengan akurasi yang lebih tinggi — peningkatan dari bagging berkurang.\n","\n","Setelah mengantongi selesai, dan semua model telah dibuat pada (kebanyakan) data yang berbeda, rata-rata tertimbang kemudian digunakan untuk menentukan skor akhir.\n","\n","![](https://miro.medium.com/max/866/1*JksRZ1E72Rsx2s8lQbNR1w.jpeg)\n","\n","Ada tiga istilah utama yang menggambarkan ensemble (kombinasi) dari berbagai model menjadi satu model yang lebih efektif:\n","\n","* **Bagging** untuk mengurangi varians model;\n","* **Boosting** untuk mengurangi bias model, dan;\n","* **Stacking** untuk meningkatkan kekuatan prediksi classifier.\n","\n","Ide di balik bagging adalah menggabungkan hasil dari beberapa model (misalnya, semua pohon keputusan) untuk mendapatkan hasil yang digeneralisasi. Inilah pertanyaannya: Jika Anda membuat semua model pada kumpulan data yang sama dan menggabungkannya, apakah akan berguna? Ada kemungkinan besar bahwa model-model ini akan memberikan hasil yang sama karena mereka mendapatkan input yang sama. Jadi bagaimana kita bisa memecahkan masalah ini? Salah satu tekniknya adalah bootstrap.\n","\n","Bootstrapping adalah teknik pengambilan sampel di mana kami membuat subset pengamatan dari dataset asli, dengan penggantian. Ukuran himpunan bagian sama dengan ukuran himpunan aslinya.\n","\n","Teknik Bagging (atau Bootstrap Aggregating) menggunakan himpunan bagian (tas) ini untuk mendapatkan gambaran yang adil tentang distribusi (set lengkap). Ukuran subset yang dibuat untuk bagging mungkin lebih kecil dari set aslinya.\n","\n","![](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/05/image20-768x289.png)\n","\n","Step 1: Multiple subsets are created from the original dataset, selecting observations with replacement.\n","\n","Step 2: A base model (weak model) is created on each of these subsets.\n","\n","Step 3: The models run in parallel and are independent of each other.\n","\n","Step 4: The final predictions are determined by combining the predictions from all the models.\n","![](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/05/Screenshot-from-2018-05-08-13-11-49-768x580.png)"]},{"cell_type":"code","execution_count":61,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.datasets import load_wine\n","# define dataset\n","X,y = load_wine().data,load_wine().target"]},{"cell_type":"code","execution_count":62,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Mean of: 0.983, std: (+/-) 0.025 [RandomForestClassifier]\n","Mean of: 0.972, std: (+/-) 0.028 [Bagging RandomForestClassifier]\n","\n","Mean of: 0.994, std: (+/-) 0.017 [ExtraTreesClassifier]\n","Mean of: 0.983, std: (+/-) 0.025 [Bagging ExtraTreesClassifier]\n","\n","Mean of: 0.675, std: (+/-) 0.070 [KNeighborsClassifier]\n","Mean of: 0.764, std: (+/-) 0.068 [Bagging KNeighborsClassifier]\n","\n","Mean of: 0.681, std: (+/-) 0.087 [SVC]\n","Mean of: 0.664, std: (+/-) 0.056 [Bagging SVC]\n","\n","Mean of: 0.983, std: (+/-) 0.025 [RidgeClassifier]\n","Mean of: 0.983, std: (+/-) 0.025 [Bagging RidgeClassifier]\n","\n"]}],"source":["# Create classifiers\n","rf = RandomForestClassifier()\n","et = ExtraTreesClassifier()\n","knn = KNeighborsClassifier()\n","svc = SVC()\n","rg = RidgeClassifier()\n","clf_array = [rf, et, knn, svc, rg]\n","for clf in clf_array:\n","    vanilla_scores = cross_val_score(clf, X, y, cv=10, n_jobs=-1)\n","    bagging_clf = BaggingClassifier(clf,max_samples=0.4, max_features=10, random_state=seed)\n","    bagging_scores = cross_val_score(bagging_clf, X, y, cv=10,n_jobs=-1)\n","    \n","    print (\"Mean of: {1:.3f}, std: (+/-) {2:.3f} [{0}]\".format(clf.__class__.__name__,vanilla_scores.mean(), vanilla_scores.std()))\n","    print (\"Mean of: {1:.3f}, std: (+/-) {2:.3f} [Bagging {0}]\\n\".format(clf.__class__.__name__,bagging_scores.mean(), bagging_scores.std()))"]},{"cell_type":"markdown","metadata":{},"source":["Di semua kecuali satu pengklasifikasi, kami memiliki varians yang lebih rendah seperti yang ditunjukkan di atas. Selain itu, akurasi pengklasifikasi semuanya meningkat kecuali untuk SVC. Sepertinya hal mengantongi ini benar-benar berfungsi.\n","\n","Jadi pengklasifikasi individu yang dikantongi (kebanyakan) lebih baik, tetapi yang mana yang kita pilih?\n","\n","**Ayo Memilih!**\n","\n","**VotingClassifier** Sklearn memungkinkan Anda untuk menggabungkan pengklasifikasi pembelajaran mesin yang berbeda, dan melakukan pemungutan suara pada label kelas yang diprediksi untuk catatan.\n","\n","Ada dua jenis pemungutan suara yang dapat Anda lakukan untuk pengklasifikasi: keras dan lunak.\n","\n","Dengan pemungutan suara yang sulit, Anda hanya perlu mayoritas pengklasifikasi untuk menentukan seperti apa hasilnya. Seperti gambar di bawah ini, berbagai model yang dikantongi ditunjukkan dengan H, dan hasil pengklasifikasi ditampilkan pada baris. Di paling kanan, H1 dan H3 memilih catatan pertama sebagai \"tidak\" (ungu) sementara H2 memilih \"ya\" (kuning). Karena 2 model memilih \"tidak\", ansambel mengklasifikasikan rekaman itu sebagai \"tidak\".\n","\n","\n","Dengan soft (berbobot), kami menghitung persentase bobot dengan masing-masing pengklasifikasi. Probabilitas kelas yang diprediksi dari setiap model untuk setiap record dikumpulkan dan dikalikan dengan bobot classifier, dan akhirnya dirata-ratakan. Label kelas akhir kemudian diturunkan dari label kelas dengan probabilitas rata-rata tertinggi.\n","\n","Pada kenyataannya bobot sulit ditemukan jika Anda hanya memberikan tebakan terbaik untuk model mana yang menurut Anda harus diberi bobot lebih atau kurang. Untuk mengatasi proses subjektif ini, persamaan optimasi linier atau jaring saraf dapat dibangun untuk menemukan bobot yang benar untuk masing-masing model untuk mengoptimalkan akurasi ensemble."]},{"cell_type":"code","execution_count":63,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.99 (+/- 0.02) [Random Forest]\n","Accuracy: 0.98 (+/- 0.03) [Extra Trees]\n","Accuracy: 0.68 (+/- 0.07) [KNeighbors]\n","Accuracy: 0.68 (+/- 0.09) [SVC]\n","Accuracy: 0.98 (+/- 0.03) [Ridge Classifier]\n","Accuracy: 0.98 (+/- 0.03) [Ensemble]\n"]}],"source":["from sklearn.ensemble import VotingClassifier\n","clf = [rf, et, knn, svc, rg]\n","eclf = VotingClassifier(estimators=[('Random Forests', rf), ('Extra Trees', et), ('KNeighbors', knn), ('SVC', svc), ('Ridge Classifier', rg)], voting='hard')\n","for clf, label in zip([rf, et, knn, svc, rg, eclf], ['Random Forest', 'Extra Trees', 'KNeighbors', 'SVC', 'Ridge Classifier', 'Ensemble']):\n","    scores = cross_val_score(clf, X, y, cv=10, scoring='accuracy')\n","    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"]},{"cell_type":"markdown","metadata":{},"source":["Dengan hasil ansambel yang dikantongi yang ditunjukkan di atas, kami memiliki peningkatan akurasi dan penurunan varians, sehingga model ensemble kami berfungsi seperti yang diharapkan setelah kami menggabungkan semua berbagai model menjadi satu.\n","\n","Sekarang kita tahu seberapa baik model kita bekerja secara individu dan bersama-sama, apakah itu benar-benar terlihat."]},{"cell_type":"markdown","metadata":{},"source":["## 2.7 Boosting <a id=\"2.7\"></a> <br>\n","![](https://miro.medium.com/max/2936/1*jbncjeM4CfpobEnDO0ZTjw.png)\n","\n","Ide utama dari boosting adalah menambahkan model tambahan ke keseluruhan model ensemble secara berurutan. Sebelumnya dengan bagging, kami rata-ratakan setiap model individual yang dibuat. Kali ini dengan setiap iterasi boosting, model baru dibuat dan model pembelajar dasar baru dilatih (diperbarui) dari kesalahan pembelajar sebelumnya.\n","\n","Algoritma menciptakan beberapa model lemah yang outputnya ditambahkan bersama untuk mendapatkan prediksi keseluruhan. Ini adalah pemodelan ensemble dari sebelumnya. Gradien yang ditingkatkan sekarang menggeser prediksi saat ini dengan mendorongnya ke target sebenarnya, dengan cara yang mirip dengan bagaimana penurunan gradien bergerak menuju nilai sebenarnya. Optimalisasi penurunan gradien terjadi pada output dari model yang bervariasi, dan bukan parameter individualnya.\n","\n","Ada berbagai metode untuk mengoptimalkan algoritma boosting.\n","Tidak seperti contoh bagging di atas, klasik boosting pembuatan subset tidak acak dan kinerja akan tergantung pada kinerja model sebelumnya. Karena, setiap subset baru yang diulang mengandung elemen yang bisa saja salah diklasifikasikan oleh model sebelumnya. Kami juga akan menggunakan pemungutan suara yang sama seperti yang kami gunakan sebelumnya untuk menggabungkan model bersama-sama."]},{"cell_type":"code","execution_count":64,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.datasets import load_wine\n","# define dataset\n","X,y = load_wine().data,load_wine().target\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"]},{"cell_type":"markdown","metadata":{},"source":["**Peningkatan adaptif** atau **AdaBoost** adalah salah satu algoritme peningkatan paling sederhana. Biasanya, pohon keputusan digunakan untuk pemodelan. Beberapa model sekuensial dibuat, masing-masing mengoreksi kesalahan dari model terakhir. AdaBoost memberikan bobot pada pengamatan yang diprediksi secara salah dan model berikutnya bekerja untuk memprediksi nilai-nilai ini dengan benar.\n","\n","Berikut adalah langkah-langkah untuk melakukan algoritma AdaBoost:\n","\n","* Awalnya, semua pengamatan dalam dataset diberi bobot yang sama.\n","* Sebuah model dibangun di atas subset data.\n","* Menggunakan model ini, prediksi dibuat pada seluruh dataset.\n","* Kesalahan dihitung dengan membandingkan prediksi dan nilai aktual.\n","* Saat membuat model berikutnya, bobot yang lebih tinggi diberikan pada titik data yang diprediksi salah.\n","* Bobot dapat ditentukan dengan menggunakan nilai kesalahan. Misalnya, semakin tinggi kesalahan, semakin banyak bobot yang diberikan untuk pengamatan.\n","* Proses ini diulang sampai fungsi kesalahan tidak berubah, atau batas maksimum jumlah penduga tercapai."]},{"cell_type":"code","execution_count":65,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["0.8333333333333334"]},"execution_count":65,"metadata":{},"output_type":"execute_result"}],"source":["ada_boost = AdaBoostClassifier(random_state=1)\n","ada_boost.fit(X_train, y_train)\n","ada_boost.score(X_test,y_test)"]},{"cell_type":"markdown","metadata":{},"source":["**Parameters**\n","\n","**base_estimators**:\n","\n","* Ini membantu untuk menentukan jenis penaksir dasar, yaitu algoritma pembelajaran mesin yang akan digunakan sebagai pembelajar dasar.\n","\n","**n_estimators**:\n","\n","* Ini mendefinisikan jumlah penduga dasar.\n","* Nilai default adalah 10, tetapi Anda harus menyimpan nilai yang lebih tinggi untuk mendapatkan kinerja yang lebih baik.\n","\n","**learning_rate**:\n","\n","* Parameter ini mengontrol kontribusi estimator dalam kombinasi akhir.\n","* Ada trade-off antara learning_rate dan n_estimators.\n","\n","**max_depth**:\n","\n","* Menentukan kedalaman maksimum estimator individu.\n","* Tune parameter ini untuk performa terbaik.\n","\n","**n_jobs**\n","\n","* Menentukan jumlah prosesor yang diizinkan untuk digunakan.\n","* Tetapkan nilai ke -1 untuk prosesor maksimum yang diizinkan.\n","\n","**random_state** :\n","\n","* Nilai integer untuk menentukan pemisahan data acak.\n","* Nilai pasti dari random_state akan selalu menghasilkan hasil yang sama jika diberikan dengan parameter dan data pelatihan yang sama.\n","\n","**Gradient Boosting or GBM** \n","\n","Ini adalah algoritma pembelajaran mesin ansambel lain yang berfungsi untuk masalah regresi dan klasifikasi. GBM menggunakan teknik boosting, menggabungkan sejumlah peserta didik yang lemah untuk membentuk peserta didik yang kuat. Pohon regresi digunakan sebagai pembelajaran dasar, setiap pohon berikutnya secara seri dibangun di atas kesalahan yang dihitung oleh pohon sebelumnya."]},{"cell_type":"code","execution_count":66,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["0.9444444444444444"]},"execution_count":66,"metadata":{},"output_type":"execute_result"}],"source":["grad_boost= GradientBoostingClassifier(learning_rate=0.01,random_state=1)\n","grad_boost.fit(X_train, y_train)\n","grad_boost.score(X_test,y_test)"]},{"cell_type":"markdown","metadata":{},"source":["**Parameters**\n","\n","**min_samples_split**\n","\n","* Defines the minimum number of samples (or observations) which are required in a node to be considered for splitting.\n","* Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n"," \n","**min_samples_leaf**\n","\n","* Defines the minimum samples required in a terminal or leaf node.\n","* Generally, lower values should be chosen for imbalanced class problems because the regions in which the minority class will be in the majority will be very small.\n","\n","**min_weight_fraction_leaf**\n","\n","* Similar to min_samples_leaf but defined as a fraction of the total number of observations instead of an integer.\n","\n","**max_depth**\n","\n","* The maximum depth of a tree.\n","* Used to control over-fitting as higher depth will allow the model to learn relations very specific to a particular sample.\n","* Should be tuned using CV.\n"," \n","**max_leaf_nodes**\n","\n","* The maximum number of terminal nodes or leaves in a tree.\n","* Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.\n","* If this is defined, GBM will ignore max_depth.\n","\n","**max_features**\n","\n","* The number of features to consider while searching for the best split. These will be randomly selected.\n","* As a thumb-rule, the square root of the total number of features works great but we should check up to 30-40% of the total number of features.\n","* Higher values can lead to over-fitting but it generally depends on a case to case scenario.\n"," \n","\n","**XGBoost** (extreme Gradient Boosting) is an advanced implementation of the gradient boosting algorithm. XGBoost has proved to be a highly effective ML algorithm, extensively used in machine learning competitions and hackathons. XGBoost has high predictive power and is almost 10 times faster than the other gradient boosting techniques. It also includes a variety of regularization which reduces overfitting and improves overall performance. Hence it is also known as ‘**regularized boosting**‘ technique.\n","\n","Let us see how XGBoost is comparatively better than other techniques:\n","\n","**Regularization:**\n","\n","Standard GBM implementation has no regularisation like XGBoost.\n","Thus XGBoost also helps to reduce overfitting.\n","\n","**Parallel Processing:**\n","* XGBoost implements parallel processing and is faster than GBM .\n","* XGBoost also supports implementation on Hadoop.\n","\n","**High Flexibility:**\n","XGBoost allows users to define custom optimization objectives and evaluation criteria adding a whole new dimension to the model.\n","\n","**Handling Missing Values:**\n","XGBoost has an in-built routine to handle missing values.\n","\n","**Tree Pruning:**\n","XGBoost makes splits up to the max_depth specified and then starts pruning the tree backwards and removes splits beyond which there is no positive gain.\n","\n","**Built-in Cross-Validation:**\n","XGBoost allows a user to run a cross-validation at each iteration of the boosting process and thus it is easy to get the exact optimum number of boosting iterations in a single run."]},{"cell_type":"code","execution_count":69,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["0.9444444444444444"]},"execution_count":69,"metadata":{},"output_type":"execute_result"}],"source":["xgb_boost=xgb.XGBClassifier(random_state=1,learning_rate=0.01, eval_metric='mlogloss')\n","xgb_boost.fit(X_train, y_train)\n","xgb_boost.score(X_test,y_test)"]},{"cell_type":"markdown","metadata":{},"source":["**Parameters**\n","\n","**nthread**\n","\n","* This is used for parallel processing and the number of cores in the system should be entered..\n","* If you wish to run on all cores, do not input this value. The algorithm will detect it automatically.\n","\n","**eta**\n","\n","Analogous to learning rate in GBM.\n","Makes the model more robust by shrinking the weights on each step.\n","\n","**min_child_weight**\n","\n","* Defines the minimum sum of weights of all observations required in a child.\n","* Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n","\n","**max_depth**\n","\n","* It is used to define the maximum depth.\n","* Higher depth will allow the model to learn relations very specific to a particular sample.\n","\n","**max_leaf_nodes**\n","\n","* The maximum number of terminal nodes or leaves in a tree.\n","* Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.\n","* If this is defined, GBM will ignore max_depth.\n","\n","**gamma**\n","\n","* A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split.\n","* Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n","\n","**subsample**\n","\n","* Same as the subsample of GBM. Denotes the fraction of observations to be randomly sampled for each tree.\n","* Lower values make the algorithm more conservative and prevent overfitting but values that are too small might lead to under-fitting.\n","\n","**colsample_bytree**\n","\n","* It is similar to max_features in GBM.\n","* Denotes the fraction of columns to be randomly sampled for each tree."]},{"cell_type":"code","execution_count":70,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.88 (+/- 0.14) [Ada Boost]\n","Accuracy: 0.91 (+/- 0.08) [Grad Boost]\n","Accuracy: 0.93 (+/- 0.07) [XG Boost]\n","Accuracy: 0.92 (+/- 0.09) [Ensemble]\n"]}],"source":["eclf = VotingClassifier(estimators=[('Ada Boost', ada_boost), ('Grad Boost', grad_boost), ('XG Boost', xgb_boost)], voting='hard')\n","clf = [rf, et, knn, svc, rg]\n","for clf, label in zip([ada_boost, grad_boost, xgb_boost,eclf], ['Ada Boost','Grad Boost','XG Boost','Ensemble']):\n","    scores = cross_val_score(clf, X, y, cv=10, scoring='accuracy')\n","    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n","   "]},{"cell_type":"markdown","metadata":{"_uuid":"62d0c612885169d9c6471c797406e6c3e0ae2bbd","collapsed":true,"trusted":true},"source":["# References<a id=\"3\"></a> <br>\n","\n","1. https://www.mygreatlearning.com/blog/ensemble-learning\n","2. https://machinelearningmastery.com/model-averaging-ensemble-for-deep-learning-neural-networks/\n","3. https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/\n","4. https://towardsdatascience.com/ensemble-methods-in-machine-learning-what-are-they-and-why-use-them-68ec3f9fef5f\n","5. https://medium.com/@rrfd/boosting-bagging-and-stacking-ensemble-methods-with-sklearn-and-mlens-a455c0c982de\n","6. https://www.toptal.com/machine-learning/ensemble-methods-machine-learning\n","\n","# Conclusion<a id=\"4\"></a> <br>\n","\n","# I hope by now you had a fair understanding of what is Ensemble Learning Methods. \n","\n","# Please do leave your comments /suggestions and if you like this kernel greatly appreciate to<font color ='red'> UPVOTE ."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":4}
